---
title: "Generation of figures and tables for the paper"
description: 
author:
  - name: Lars Relund Nielsen
    url: http://pure.au.dk/portal/en/larsrn@econ.au.dk
    affiliation: CORAL, BSS, Aarhus University
    affiliation_url: https://econ.au.dk/coral
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

```{r knitr setup, include = FALSE}
library(knitr)
library(kableExtra)
library(formatR)
# setupKnitr()
# knitr::opts_chunk$set(
#   collapse = TRUE,
#   comment = "#>",
#   fig.path = "ublb-",
#   warning=FALSE, message=FALSE, include = TRUE, 
#   out.width = "99%", fig.width = 8, fig.align = "center", fig.asp = 0.62
# )
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning=FALSE, message=FALSE, include = TRUE, 
  # cache = TRUE, autodep = TRUE,
  echo=F, #results = "hide",
  out.width = "99%", fig.width = 8, fig.align = "center", fig.asp = 0.7,
  tidy.opts = list(width.cutoff = 80), tidy = F
  # layout="l-page"   #"l-screen-inset"
)
options(knitr.kable.NA = '', knitr.table.format = "latex", width = 80)
```

```{r setup, include=FALSE}
library(gMOIP)
library(here)
library(ggsci)
library(tidyverse)
library(tikzDevice)
library(magrittr)
library(RColorBrewer)
library(wesanderson)
library(ggthemes)
library(rmarkdown)
here::i_am(path = "results/paper_tab_fig/fig_tab.Rmd")
pathOverLeaf <- "/Users/au15463/Dropbox/Apps/Overleaf/LP_relax_based_BB"
copyTabFigs <- FALSE   # copy to Overleaf
if (isTRUE(getOption('knitr.in.progress'))) copyTabFigs <- TRUE 
```

```{r snippet function}
save_snippet <- function(name, str, path = here(pathOverLeaf, "snippets.tex"), append = T) {
  str <- str_c("\\newcommand{\\", name, "}{", str, "}\n")
  write_lines(str, path, append = append)
}
```

```{r color scale palette used, include=FALSE}
# https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html
library("scales")
mypal <- pal_npg("nrc", alpha = 0.8)(9)
show_col(mypal)
mypal <- pal_simpsons()(9)
show_col(mypal)
```

```{r Define theme and color scales, include=FALSE}
theme_publish <- function() {
  library(grid)
  library(ggthemes)
  return(theme_foundation(base_size = 10) +
    theme(
      # plot.title = element_text(face = "bold", size = rel(1.2), hjust = 0.5),
      text = element_text(face = "plain"),
      # panel.background = element_rect(colour = NA),
      panel.spacing = unit(0.5, "cm"),
      plot.background = element_blank(),
      # panel.border = element_rect(colour = NA),
      axis.title = element_text(face = "plain",size = rel(1)),
      # axis.title.y = element_text(angle=90,vjust =2),
      # axis.title.x = element_text(vjust = -0.2),
      axis.text = element_text(),
      axis.line = element_line(colour="black"),
      axis.ticks = element_line(),
      panel.grid.major = element_line(colour="#f0f0f0"),
      panel.grid.minor = element_blank(),
      legend.key = element_rect(colour = NA),
      legend.position = "bottom",
      legend.direction = "horizontal",
      # legend.key.size = unit(0.2, "cm"),
      legend.key.height = unit(0.2, "cm"), 
      legend.key.width = unit(0.75, "cm"),
      # legend.margin = unit(0, "cm"),
      # legend.title = element_blank(), #element_text(face="italic"),
      # plot.margin=unit(c(10,5,5,5),"mm"),
      # strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
      # strip.text = element_text(face="bold"),
    )
  )
}
```

```{r load data}
datInput <- read_csv(here("results", "paper_tab_fig", "instances.csv"))

rules <- c("med2", "rand", "mofvRevisited2")  # rules to compare
datResult <- read_csv(here("results", "convert", "csv", "resultsMain.csv")) %>%
  filter(configValSplit %in% rules) %>%   # remove old rules
  mutate(
    instance_name = str_remove(instance, "....$"),
    class = str_replace(instance_name, "^.*?-(.*?)_(.*)", "\\1"),
    alg_config = str_c("\\", configLB, "-\\", toupper(configValSplit)),
    configValSplit = case_when(   # rename rules for paper
      (class == "KP" | class == "UFLP") ~ "binary",
      configValSplit == "med2" ~ "med",
      configValSplit == "mofvRevisited2" ~ "mofv",
      TRUE ~ configValSplit
    )) %>% 
  group_by(instance_name) %>% 
  nest() %>% 
  mutate(winner = map(data, function(df) {df %>% arrange(cpuTotal) %>% slice(1)})) %>% 
  mutate(win = map(winner, function(df) {if_else(df$solved == 1, df$alg_config, "Unsolved")}),
         win_bound = map(winner, function(df) {if_else(df$solved == 1, toupper(df$configValSplit), "Unsolved")}),
  ) %>% 
  select(-winner) %>% 
  unnest(c(data, win, win_bound)) 

# results where at least on config solved the instance
datResultSolvedI <- datResult %>% 
  group_by(instance_name) %>% 
  nest() %>% 
  mutate(sol = map(data, function(df) {any(df$solved == 1)})) %>% 
  unnest(sol) %>% 
  filter(sol) %>% 
  select(-sol) %>% 
  unnest(data) 

datResultAllSolved <- datResult %>% 
  group_by(instance_name) %>% 
  nest() %>% 
  mutate(sol = map(data, function(df) {all(df$solved == 1)})) %>% 
  unnest(sol) %>% 
  filter(sol) %>% 
  select(-sol) %>% 
  unnest(data) 

datLBStat <- read_csv(here("results/paper_tab_fig/lb_set.csv"))

datOSS <- read_csv(here("results/convert/csv/resultsOSS.csv"))
```

## Test instances

### Instance table

Will be copied to Overleaf directly via DB:

```{r Table with problem classes}
res <- datResult 
tabInput <- nest_join(datInput, res) %>% 
  mutate(nd = map_dbl(res, function(df) {
    if (any(df$solved == 1)) 
      return(df %>% filter(solved == 1) %>% pull(YN) %>% mean())
    return(df %>% pull(YN) %>% mean())
  })) %>% 
  group_by(class, n, p) %>% 
  summarise(instances = n(), 
            range_coeff_min = min(range_coeff_min), 
            range_coeff_max = max(range_coeff_max),
            coef_nd_pct = mean(coef_nd_pct),
            const_mat_non_zeros = mean(const_mat_non_zeros),
            range_int_min = min(range_int_min),
            range_int_max = max(range_int_max),
            n_binary = mean(n_binary),
            n_integer = mean(n_integer),
            nd = mean(nd)
  )  %>%  
  group_by(class, p) %>% 
  summarise(n = str_c(n, collapse = ", "), 
            coef_nd_pct = 100*mean(coef_nd_pct),
            const_mat_non_zeros = 100*mean(const_mat_non_zeros),
            range_coeff = str_c("[", min(range_coeff_min), ",", max(range_coeff_max), "]"),
            range_int_var = str_c("[", min(range_int_min), ",", max(range_int_max), "]"),
            instances = sum(instances),
            nd = str_c(round(nd, 0), collapse = ", ")
  )  

table <- tabInput %>% 
  relocate(range_coeff, coef_nd_pct, .after = n) %>% 
  select(-range_int_var, -nd) %>% 
  kable(format = "latex",
        position = "tb",
        align = c("X","C","L","Z","Z","Z","Z"),
        escape = F,
        label = "instances",
        #table.envir = NULL,
        booktabs = T,
        digits = 0,
        linesep = "",
        col.names = c("Class", 
                      str_c("$p$", footnote_marker_alphabet(1)), 
                      str_c("$n$", footnote_marker_alphabet(2)), 
                      str_c("$[C]$", footnote_marker_alphabet(3)),
                      str_c("$\\% C$", footnote_marker_alphabet(4)),
                      str_c("$\\% A$", footnote_marker_alphabet(5)),
                      # str_c("$[\\N]$", footnote_marker_alphabet(6)),
                      str_c("\\#", footnote_marker_alphabet(6))
                      # str_c("$|\\YN|$", footnote_marker_alphabet(7))
                    ),
        caption = "Instances used and average number of non-dominated points.") %>% 
  kable_styling(latex_table_env = "tabularx") %>%
  # kable_styling(latex_options = "scale_down") %>%
  footnote(alphabet = c("Number of objectives.", 
                         "Number of variable sizes.", 
                         "Range of the objective function coefficients $C$.",
                         "Percentage of objective coefficients not dominated by other coefficients.",
                         "Percentage of non-zeros in the constraint matrix $A$.",
                         "Number of instances."
                         # "Average number of non-dominated points for each variable size."
                        ), 
           footnote_as_chunk = F, escape = F, fixed_small_size = T) %>% 
  str_replace("centering", "tiny") %>% 
  str_replace(fixed("(\\#tab:instances)"), "\\label{tab:instances}") %>% 
  str_replace("begin\\{tabularx\\}", "begin\\{tabularx\\}\\{\\\\linewidth\\}") 
table %>% kable(format = "html")

if (copyTabFigs) {
  table %>% write_file(file = here(pathOverLeaf, "tab_instances.tex"))
}
```

### Non-dominted points

Will be copied to Overleaf directly via DB:

```{r}
res <- datResult 
dat <- nest_join(datInput, res) %>% 
  mutate(nd = map_dbl(res, function(df) {
    if (any(df$solved == 1)) 
      return(df %>% filter(solved == 1) %>% pull(YN) %>% mean())
    return(df %>% pull(YN) %>% mean())
  }), 
  solved = map_chr(res, function(df) {
    if (any(df$solved == 1)) return("solved") else return("reached time limit")
  })) %>% 
  select(class,instance_name, n, nd, solved, p) %>% print

scale_alpha_solved <- scale_alpha_manual(
  values = c("solved" = 1, "reached time limit" = 0.25),
  drop = F)

p <- ggplot(aes(x = n, y = nd), data = dat) +
  geom_point(aes(alpha = solved, color = factor(p)), size = 1) + 
  geom_smooth(aes(color = factor(p)), se = F) +
  facet_wrap(facets = vars(class), scales = 'free') +
  xlab("variable size ($n$)") + ylab("number of non-dominated points") +
  labs(color = "$p$", linetype = "$p$", alpha = "") +
  theme_publish() +
  scale_alpha_solved
p
if (copyTabFigs) {
  tikz(file = here(pathOverLeaf, "fig_nd.tex"), standAlone=F, width = 7, height = 4)
  print(p)
  dev.off()
}
```

```{r}
str <- str_glue("\\comment[id=LRN]{{Generated using R. Don't modify here.}}In \\autoref{{fig:nd}} the number of non-dominated points are given for each instance. We have increased the variable size for each problem class until the size becomes so large that some instances cannot be solved within the time limit. The instances which have not been solved to optimality ({res}\\%) are illustrated with transparent points. In general the number of non-dominated points grow with variable size ($n$) and number of objectives ($p$). Note though that there may be a high variation for fixed $n$ and $p$. Moreover, the variation grows with $n$ and $p$. For UFLP the number of non-dominated points grows rapidly as a function of variable size which is due to the high percentage of objective coefficients not dominated by other coefficients.",
              res = round(length(which(dat$solved != "solved"))/nrow(dat)*100))
save_snippet("snipInstancesA", str, append = F)
```







## Performance of the different algorithm configurations

```{r}
dat <- datResultSolvedI %>% 
  group_by(instance_name) %>% 
  filter(setequal(unique(configValSplit), rules)) %>%    # only instances with results for for all rules
  group_by(alg_config) %>% 
  summarize(cpu = mean(cpuTotal), .groups = "drop") %>% 
  arrange(cpu) %>% 
  mutate(pct = round(100*(cpu/min(cpu) - 1)))
```

```{r}
str <- str_glue("\\comment[id=LRN]{{Generated using R. Don't modify here.}}First, we rank the configurations with respect to the average cpu time for all solved instances. The sequence from best to worst becomes {res}, where the increase in percentages compared to the best configuration is given in parentheses.",
                res = knitr::combine_words(str_c(dat$alg_config, " (", dat$pct, "\\%)")))
save_snippet("snipAlgConfigA", str)
```


### Performance plot

Will be copied to Overleaf directly via DB.

```{r perfPlotPct}
dat <- datResult %>%
  # group_by(instance_name) %>% 
  # filter(setequal(unique(configValSplit), rules)) %>%    # only instances with results for for all rules
  group_by(class, p, alg_config) %>%
  arrange(cpuTotal, .by_group = T) %>%
  mutate(count = row_number(), 
         total = n(), 
         pct = count/total, 
         configValSplit = toupper(configValSplit)) %>% 
  select(p, class, alg_config, cpuTotal, count, pct, total, configLB, configValSplit)
  
  # group_by(class, p, configLB) %>%
  # arrange(cpuTotal) %>%
  # mutate(count = row_number(), total = n()) %>% 
  # group_by(class, p) %>% 
  # mutate(total = max(total)) %>% 
  # group_by(class, alg_config, cpuTotal) %>% 
  # arrange(class, alg_config, cpuTotal, count) %>% 
  # filter(cpuTotal < 3600 | row_number() == 1) %>% 
  # mutate(pct = count/total, configValSplit = toupper(configValSplit)) %>% 
  # select(p, class, alg_config, cpuTotal, count, pct, total, configLB, configValSplit)



p <- ggplot(dat) +
  # geom_step(aes(x=tpstotal, y=count, color = nodeselVarsel, linetype = OB)) +
  geom_step(aes(y=pct, x=cpuTotal, color = configLB, linetype = configValSplit), alpha = 0.75) +
  # geom_point(aes(y=..y.., x=cpuTotal, color = configLB, linetype = configValSplit), stat="ecdf", size = 0.5) +
  facet_grid(rows = vars(class), cols = vars(p)) +
  # ggtitle(str_c("Number of instances solved within a given cpu time")) +
  labs(color = "", linetype = "") +
  # scale_color_ob + #scale_linetype_nodesel_varsel +
  xlab("cpu (seconds)") + ylab("\\%") +
  theme_publish() +
  coord_cartesian(expand = FALSE, ylim = c(0, NA), xlim = c(-10, 3595))
p
if (copyTabFigs) {
  tikz(file = here(pathOverLeaf, "fig_pref_pct.tex"), standAlone=F, width = 7, height = 6)
  print(p)
  dev.off()
}
```

```{r perf stat for text}
dat <- datResultSolvedI %>% 
  group_by(instance, win) %>% 
  nest() %>% 
  mutate(win_wlp = str_detect(win, "WLP")) %>% 
  group_by(win_wlp) %>% 
  summarize(ctr = n()) 
str <- str_glue(
  "\\comment[id=LRN]{{Generated using R. Don't modify here.}}Second, a comparison of the different algorithm configurations for a given problem class and objectives can be seen in \\autoref{{fig:performanceProfiles}}. We have increased the variable size for each problem class until the size becomes so large that some instances cannot be solved within the time limit. That is, the number of instances solved before the time limit is below 100\\%. Note that \\WLP in general performs best (fastest in {res}\\% of the instances).",
  res = round(100 * dat %>% filter(win_wlp) %>% pull(ctr)/sum(dat$ctr))
  )

dat <- datResultSolvedI %>%
  group_by(instance_name, configLB, class) %>%
  summarise(cpu = mean(cpuTotal, na.rm = T)) %>% 
  pivot_wider(names_from = configLB, values_from = cpu) %>% 
  mutate(cpu_pct = 1- WLP/LP) %>% 
  group_by(class) %>% 
  summarise(cpu_pct = round(100 * mean(cpu_pct)))
str <- str_c(str, str_glue(
  "If we compare problem classes \\WLP performs well for all classes with an average cpu time reduction of {combine_words(res)} for {combine_words(dat$class)}, respectively.",
  res = str_c(dat$cpu_pct, "\\%")
), sep = " ")

dat <- datResultSolvedI %>% 
  group_by(instance_name, win, class) %>% 
  nest() %>% 
  mutate(win_wlp = str_detect(win, "WLP")) %>% 
  filter(!win_wlp) %>% 
  unnest(data) %>% 
  filter(win == alg_config) %>% 
  group_by(class) %>% 
  summarise(ctr = n(), cpu = round(mean(cpuTotal))) 
str <- str_c(str, str_glue(
  "In a few instances \\LP performs best. This happens for {dat %>% filter(class == 'ILP') %>% pull(ctr)} instances in ILP with an average cpu time of {dat %>% filter(class == 'ILP') %>% pull(cpu)} seconds. We will have a closer look at reason for this in \\autoref{{sec:resDetails}}."
), sep = " ")

save_snippet("snipAlgConfigB", str)
```



## Variable selection - Rules for choosing the bound

```{r pref bound}
rules <- c("med", "rand", "mofv")  # rules to compare
dat <- datResultSolvedI %>% #filter(instance_name == "Kirlik14-ILP_p-3_n-30_m-15_ins-3") %>%
  filter(configValSplit %in% rules) %>% 
  filter(configLB == "WLP") %>% 
  group_by(instance_name) %>% 
  filter(setequal(unique(configValSplit), rules))   # only instances with results for for all rules
  
# dat1 <- dat %>% 
#   group_by(class, configValSplit) %>%
#   summarize(instances = n(), cpu = mean(cpuTotal), nodes = mean(nbNodes)) %>% 
#   arrange(nodes) %>% print
# 
# dat1 <- dat %>% 
#   group_by(configValSplit) %>%
#   summarize(instances = n(), cpu = mean(cpuTotal), nodes = mean(nbNodes)) %>% 
#   arrange(nodes) %>% print
  
dat <- dat %>% ungroup() %>% 
  group_by(instance_name) %>% 
  arrange(cpuTotal, .by_group = T) %>% 
  slice_head(n = 2) %>%  
  mutate(diff = cpuTotal-min(cpuTotal),
         pct = round(100*(cpuTotal/min(cpuTotal) - 1))) %>% 
  select(instance_name, diff, pct, cpuTotal, configValSplit, win_bound, class) %>% 
  arrange(cpuTotal, .by_group = T) %>% #print %>% 
  group_by(win_bound) %>%
  summarize(ctr = n(), diff = mean(diff), pct = mean(pct)) %>%
  # group_by(configValSplit) %>% 
  mutate(ctr_pct = round(100 * ctr/sum(ctr))) %>% 
  arrange(desc(ctr_pct), .by_group = T) %>% print() %>% 
  ungroup() %>% 
  slice_head(n = 1)
str <- str_glue(
  "\\comment[id=LRN]{{Generated using R. Don't modify here.}}We are here interested in determining whether one rule for finding the bound ({res1}) is consistently better than the other when considering non-binary integer problems. Since the effect on the branching tree of using \\LP vs \\WLP is negligible, we will consider the \\WLP configuration here. By considering the performance profiles in \\autoref{{fig:performanceProfiles}} we see that there is no clear winner among {res1}. 
  
The {dat$win_bound}-configuration performed best in {dat$ctr_pct}\\% of the instances. If we compare with the second best rule for each instance, the cpu time on average increased with {round(mean(dat$diff))} seconds (a {round(100*mean(dat$pct))}\\% increase).",
  res1 = knitr::combine_words(toupper(rules))
  )
save_snippet("snipBoundA", str)
```



### Bound rule figure
  
Will be copied to Overleaf directly via DB:

```{r cpu plot}
dat <- datResult %>% #filter(instance_name == "Kirlik14-ILP_p-3_n-30_m-15_ins-3") %>%
  filter(configValSplit %in% rules) %>% 
  filter(configLB == "WLP") %>% 
  group_by(instance_name) %>% 
  filter(setequal(unique(configValSplit), rules)) %>%    # only instances with results for for all rules
  mutate(configValSplit = toupper(configValSplit))
         
p <- ggplot(aes(x = n, y = cpuTotal, color = configValSplit), data = dat) +
  geom_point(size = 1) +
  geom_smooth(se = F, size = 0.5) +
  facet_grid(rows = vars(class), cols= vars(p), scales = "free", space = "free") +
  # ggtitle(str_c("Number of instances solved within a given cpu time")) +
  labs(color = "", linetype = "") +
  # scale_color_ob + #scale_linetype_nodesel_varsel +
  xlab("Number of variables") + ylab("Cpu (seconds)") +
  theme_publish() 
  # coord_cartesian(expand = FALSE, ylim = c(0, NA), xlim = c(-10, NA))
p
if (copyTabFigs) {
  tikz(file = here(pathOverLeaf, "fig_bound.tex"), standAlone=F, width = 7, height = 5)
  print(p)
  dev.off()
}
```


```{r bound table, eval=FALSE}
dat1 <- datResultSolvedI %>% #filter(instance_name == "Kirlik14-ILP_p-3_n-30_m-15_ins-10") %>% view()
  filter(class != "UFLP") %>%   # remove binary
  group_by(instance, win) %>%
  mutate(win_lb = if_else(str_detect(win, "WLP"), "WLP", "LP"),
         win_bound = case_when(
           str_detect(win, "MED") ~ "med",
           str_detect(win, "MOFV") ~ "mofv",
           str_detect(win, "RAND") ~ "rand",
           TRUE ~ "error"
         )) %>% 
  filter(configLB == win_lb) %>% # only consider the win LB config
  group_by(instance_name) %>% 
  nest() %>% 
  mutate(sol = map(data, function(df) {all(df$solved == 1)})) %>% 
  unnest(sol) %>% 
  filter(sol) %>% # remove instances with unsolved configs
  select(-sol) %>% 
  unnest(data) %>% 
  select(instance_name, configValSplit, cpuTotal, class, p) %>%
  group_by(instance_name) %>%
  pivot_wider(names_from = configValSplit, values_from = c(cpuTotal), values_fill = NA, values_fn = {mean}) %>%
  group_by(class, p) %>%
  summarise(avgSpeedup = mean(med/mofv)) %>%
  ungroup() %>%
  pivot_wider(names_from = p, values_from = avgSpeedup)

dat2 <- datResultSolvedI %>% 
  filter(class != "UFLP") %>%   # remove binary
  group_by(instance, win) %>%
  mutate(win_lb = if_else(str_detect(win, "WLP"), "WLP", "LP"),
         win_bound = if_else(str_detect(win, "MED"), "med", "mofv")) %>% 
  filter(configLB == win_lb) %>% # only consider the win LB config
  group_by(instance_name) %>% 
  nest() %>% 
  mutate(sol = map(data, function(df) {all(df$solved == 1)})) %>% 
  unnest(sol) %>% 
  filter(sol) %>% # remove instances with unsolved configs
  select(-sol) %>% 
  unnest(data) %>% 
  select(instance_name, configValSplit, nbNodes, class, p) %>%
  group_by(instance_name) %>%
  pivot_wider(names_from = configValSplit, values_from = c(nbNodes)) %>% #filter(mofv > med)
  group_by(class, p) %>%
  summarise(avgSpeedup = mean(med/mofv)) %>%
  ungroup() %>%
  pivot_wider(names_from = p, values_from = avgSpeedup)

dat <- bind_cols(dat1,dat2 %>% select(-class)) %>% 
  mutate(across(where(is.numeric), ~format(round(.x, 2), nsmall = 2)))

table <- dat %>% 
  kable(format = "latex",
        position = "tb",
        align = c("X","Y","Y","Y","Y","Y","Y"),
        escape = F,
        label = "factorBound",
        #table.envir = NULL,
        booktabs = T,
        digits = 0,
        linesep = "",
        col.names = c("Class",
                      str_c("$p=3$"), str_c("$p=4$"), str_c("$p=5$"),
                      str_c("$p=3$"), str_c("$p=4$"), str_c("$p=5$")
                    ),
        caption = "Speed-up factor of using \\MOFV instead of \\MED for each problem class and number of objectives.") %>% 
  kable_styling(latex_table_env = "tabularx") %>%
  add_header_above(c(" " = 1, "Cpu" = 3, "Branching tree size" = 3))%>% 
  # kable_styling(latex_options = "scale_down") %>% 
  str_replace("begin\\{tabularx\\}", "begin\\{tabularx\\}\\{\\\\linewidth\\}") %>% 
  str_replace("centering", "footnotesize") %>% 
  str_replace(fixed("(\\#tab:factorBound)"), "\\label{tab:factorBound}")
table

if (copyTabFigs) {
  table %>% write_file(file = here(pathOverLeaf, "tab_bound.tex"))
}
```


## Detailed performance of different algorithm parts

Will be copied to Overleaf directly via DB:

```{r factor lp/wlp table}
dat <- datResultSolvedI %>% 
  filter(configValSplit == "mofv" | class == "UFLP" | class == "KP") %>% 
  group_by(instance_name) %>%
  nest() %>% 
  mutate(sol = map(data, function(df) {all(df$solved == 1)})) %>% 
  unnest(sol) %>% 
  filter(sol) %>% # remove instances with unsolved configs
  select(-sol) %>% 
  unnest(data) %>% 
  select(instance_name, configLB, cpuTotal, nbLpSolved, nbNodes, class, p) %>%
  group_by(instance_name) %>%
  pivot_wider(names_from = configLB, values_from = c(cpuTotal, nbLpSolved, nbNodes), values_fn = mean) %>% 
  ungroup() %>%
  group_by(class, p) %>%
  summarise(avgSpeedup = mean(cpuTotal_LP/cpuTotal_WLP), avgLP = mean(nbLpSolved_LP/nbLpSolved_WLP)) %>%
  ungroup() %>%
  pivot_wider(names_from = p, values_from = c(avgSpeedup, avgLP))

table <- dat %>% 
  kable(format = "latex",
        position = "tb",
        align = c("X","Y","Y","Y","Y","Y","Y"),
        escape = F,
        label = "factorLB",
        #table.envir = NULL,
        booktabs = T,
        digits = 2,
        linesep = "",
        col.names = c("Class",
                      str_c("$p=3$"), str_c("$p=4$"), str_c("$p=5$"),
                      str_c("$p=3$"), str_c("$p=4$"), str_c("$p=5$")
                    ),
        caption = "Speed-up factor using \\WLP instead of \\LP for each problem class and number of objectives. The rule for choosing the bound is \\MOFV.") %>% 
  kable_styling(latex_table_env = "tabularx") %>%
  add_header_above(c(" " = 1, "Cpu" = 3, "LPs solved" = 3))%>% 
  # kable_styling(latex_options = "scale_down") %>% 
  str_replace("begin\\{tabularx\\}", "begin\\{tabularx\\}\\{\\\\linewidth\\}") %>% 
  str_replace("centering", "footnotesize") %>% 
  str_replace(fixed("(\\#tab:factorLB)"), "\\label{tab:factorLB}")
table

if (copyTabFigs) {
  table %>% write_file(file = here(pathOverLeaf, "tab_factor_lb.tex"))
}
```

```{r detailed text}
dat <- datResultSolvedI %>% 
  filter(configValSplit == "mofv" | class == "UFLP") %>% 
  group_by(instance_name) %>%
  nest() %>% 
  mutate(sol = map(data, function(df) {all(df$solved == 1)})) %>% 
  unnest(sol) %>% 
  filter(sol) %>% # remove instances with unsolved configs
  select(-sol) %>% 
  unnest(data) %>% 
  select(instance_name, configLB, cpuTotal, nbLpSolved, nbNodes, class, p) %>%
  group_by(instance_name) %>%
  pivot_wider(names_from = configLB, values_from = c(cpuTotal, nbLpSolved, nbNodes), values_fn = mean) %>% 
  ungroup() %>%
  group_by(class, p) %>%
  summarise(avgSpeedup = mean(cpuTotal_LP/cpuTotal_WLP), avgLP = mean(nbLpSolved_LP/nbLpSolved_WLP))
dat2 <- datResult %>%
  filter(configValSplit == "mofv" | class == "UFLP", solved == 1) %>%
  mutate(
    pctLB = 100 * cpuLbComputation / cpuTotal ,
    pctUB = 100 * cpuUbUpdate / cpuTotal ,
    pctDomiTest = 100 * cpuDominanceTest / cpuTotal ,
    pctNodeSel = 100 * cpuNodeSel / cpuTotal ,
    pctVarSel = 100 * cpuVarSel / cpuTotal ,
    pctOther = 100 - pctLB - pctUB - pctDomiTest - pctNodeSel - pctVarSel
  ) %>%
  group_by(p, configLB, class) %>%
  summarise(
    `Calculation of LB` = mean(pctLB) ,
    `Updating UB` = mean(pctUB) ,
    `Pruning nodes` = mean(pctDomiTest) ,
    Other = mean(pctOther) + mean(pctNodeSel) + mean(pctVarSel)
  )
str <- str_glue("\\comment[id=LRN]{{Generated using R. Don't modify here.}}In this section, we take a closer look at different parts of \\autoref{{alg:BB}}. Different speed-up factors by using \\WLP instead of \\LP are given in \\autoref{{tab:factorLB}}. The factor is obtained by dividing the \\LP value with the \\WLP value. Only instances with both configurations solved are recorded. \\WLP are on average {res1} times faster than \\LP with significant differences among the problem classes, \\eg for problem class UFLP, \\WLP is on average {res2} times faster while for class ILP the speed-up is {res3}.
                
Most of the cpu time ({res4a}\\% for \\LP and {res4b}\\% for \\WLP) is used on calculating the lower bound set (\\autoref{{alg:benson}}) and the speed-up is mainly due to a reduction in the number of times the LP solver has to be called on \\autoref{{l:checkV}} in \\autoref{{alg:benson}}. This can be seen in \\autoref{{tab:factorLB}}. For example, for UFLP \\WLP is {res2} times faster and solved {res5} times less LPs on average than \\LP. However, when using \\WLP the initial outer approximation has to be copied from the father node into the child node and managing the polyhedron is harder since we have to check for redundant half-spaces in \\autoref{{alg:updateP}} (lines \\ref{{l:sRemoveH}}-\\ref{{l:eRemoveH}}). As a result we have a smaller reduction in cpu times compared to the reduction in tree size.",
  res1 = round(mean(dat$avgSpeedup), 2),
  res2 = round(dat %>% filter(class == "UFLP") %>% pull(avgSpeedup) %>% mean(), 2),
  res3 = round(dat %>% filter(class == "ILP") %>% pull(avgSpeedup) %>% mean(), 2),
  res4a = round(dat2 %>% filter(configLB == "LP") %>% pull(`Calculation of LB`) %>% mean),
  res4b = round(dat2 %>% filter(configLB == "WLP") %>% pull(`Calculation of LB`) %>% mean),
  res5 = round(dat %>% filter(class == "UFLP") %>% pull(avgLP) %>% mean(), 2)
  )
save_snippet("snipDetailsA", str)
```

```{r parts BB fig, eval=FALSE}
p <- datResult %>%
  filter(configValSplit == "mofv" | class == "UFLP", solved == 1) %>%
  mutate(
    pctLB = 100 * cpuLbComputation / cpuTotal ,
    pctUB = 100 * cpuUbUpdate / cpuTotal ,
    pctDomiTest = 100 * cpuDominanceTest / cpuTotal ,
    pctNodeSel = 100 * cpuNodeSel / cpuTotal ,
    pctVarSel = 100 * cpuVarSel / cpuTotal ,
    pctOther = 100 - pctLB - pctUB - pctDomiTest - pctNodeSel - pctVarSel
  ) %>%
  group_by(p, configLB, class) %>%
  summarise(
    `Calculation of LB` = mean(pctLB) ,
    `Updating UB` = mean(pctUB) ,
    `Pruning nodes` = mean(pctDomiTest) ,
    Other = mean(pctOther) + mean(pctNodeSel) + mean(pctVarSel)
  ) %>%
  pivot_longer(!c(class, p, configLB) , names_to = "part" , values_to = "pctCpu") %>%
  ggplot(aes(
    x = factor(configLB),
    y = pctCpu,
    fill = part
  )) +
  geom_col(color = "black") +
  geom_text(
    aes(label = round(pctCpu, 1)),
    colour = "white",
    size = 2.5,
    position = position_stack(vjust = .5)
  ) +
  facet_grid(
    rows = vars(class) ,
    cols = vars(p),
    margins = F,
    # scales = "free"
  ) +
  labs(color = "", fill = "") +
  ylab("percent of total cpu time") + xlab("") +
  theme_publish()
p

if (copyTabFigs) {
  tikz(file = here(pathOverLeaf, "fig_part_bb.tex"), standAlone=F, width = 7, height = 6)
  print(p)
  dev.off()
}
```

Will be copied to Overleaf directly via DB:

```{r, eval=FALSE}
dat <- datResult %>%
  filter(configValSplit == "mofv" | class == "UFLP", solved == 1) %>%
  group_by(p, class, configLB) %>% 
  summarise(poly = mean(cpuUpdatePolyhedron)) %>% print() %>% 
  group_by(configLB) %>% 
  summarise(poly = mean(poly)) %>% print
  
  
  
  mutate(cpuLbComputation = cpuLbComputation - cpuInitialization) %>% 
  
  mutate(
    pctCplex = 100 * cpuCplex / cpuLbComputation ,
    pctUpdatePolyhedron = 100 * cpuUpdatePolyhedron / cpuLbComputation ,
    pctOther = 100 - pctCplex - pctUpdatePolyhedron
  ) %>%
  group_by(p, class, configLB) %>%
  summarise(
    `Solve LPs` = mean(pctCplex) ,
    `Update polyhedron` = mean(pctUpdatePolyhedron) ,
    Other = mean(pctOther)
  ) %>%
  pivot_longer(!c(p, class, configLB) , names_to = "part" , values_to = "pctCpu") 

dat %>% group_by(p, class, configLB) %>% summarise(pct = sum(pctCpu))
dat %>% group_by(p, class, configLB) %>% summarise()
```


```{r parts benson fig}
dat <- datResult %>%
  filter(configValSplit %in% c("mofv", "binary"), solved == 1) %>%
  mutate(
    pctCplex = 100 * cpuCplex / cpuLbComputation ,
    pctUpdatePolyhedron = 100 * cpuUpdatePolyhedron / cpuLbComputation ,
    pctInitialization = 100 * cpuInitialization / cpuLbComputation,
    pctOther = 100 - pctCplex - pctUpdatePolyhedron - pctInitialization
  ) %>% 
  group_by(p, class, configLB) %>%
  summarise(
    `Solve LPs` = mean(pctCplex) ,
    `Update polyhedron` = mean(pctUpdatePolyhedron) ,
    `Initialization` = mean(pctInitialization) ,
    Other = mean(pctOther)
  ) %>%
  pivot_longer(!c(p, class, configLB) , names_to = "part" , values_to = "pctCpu") 
p <- dat %>% ggplot(aes(
    x = factor(configLB),
    y = pctCpu,
    fill = part
  )) +
  geom_col(color = "black") +
  geom_text(
    aes(label = round(pctCpu, 1)),
    colour = "white",
    size = 2.5,
    position = position_stack(vjust = .5)
  ) +
  facet_grid(
    rows = vars(class) ,
    cols = vars(p),
    margins = F,
    # scales = "free"
  ) +
  labs(color = "", fill = "") +
  ylab("percent of total cpu time") + xlab("") +
  theme_publish()
p

if (copyTabFigs) {
  tikz(file = here(pathOverLeaf, "fig_part_benson.tex"), standAlone=F, width = 7, height = 6)
  print(p)
  dev.off()
}
```


## Polyhedral properties of the lower bound set during the algorithm

```{r}
# dat <- datLBStat %>% 
#   # filter(configLB == "WLP") %>% 
#   transmute(instance_name, class, configLB, configValSplit, depth, p, avgFacetsNoRay, avgFacet,
#             # facetsLB = avgFacetsNoRay, 
#             facetsWRays = avgFacet - avgFacetsNoRay, 
#             avgVtxNoRay,
#             verticesF = avgVtxNoRay/avgFacetsNoRay) %>%  view

dat <- datLBStat %>% 
  # filter(configLB == "WLP") %>% 
  group_by(class, configLB, configValSplit, depth, p) %>% 
  summarise(avgFacetsLB = mean(avgFacetsNoRay), 
            avgFacetsWRays = mean(avgFacet - avgFacetsNoRay), 
            avgVertices = mean(avgVtxNoRay),
            avgVerticesFacetsLB = mean(if_else(avgFacetsNoRay == 0, 0, avgVtxNoRay/avgFacetsNoRay)),
            pct = mean(avgFeasVtx/avgVertex)  # proportion of old vertices and rays in the polyhederon
        ) %>% view
  
dat %>%  
  select(-avgVerticesFacetsLB, -pct) %>% 
  pivot_longer(cols = starts_with("avg")) %>% 
  ggplot(aes(x = depth, y = value, color = name, linetype = configLB)) +
  geom_line() +
  facet_grid(rows = vars(class, configValSplit), cols = vars(p), scales = "free")
  # facet_wrap(vars(class, configValSplit, p), scales = "free")

dat %>%  
  select(class, configLB, configValSplit, depth, p, avgVerticesFacetsLB) %>% 
  pivot_longer(cols = starts_with("avg")) %>% 
  ggplot(aes(x = depth, y = value, color = name, linetype = configLB)) +
  geom_line() +
  facet_grid(rows = vars(class, configValSplit), cols = vars(p), scales = "free")

pal <- brewer.pal(n = 11, name = "RdYlBu")
p1 <- dat %>%  
  filter(configValSplit == "MOFVREVISITED2", configLB == "WLP") %>% 
  select(-avgVerticesFacetsLB) %>% 
  pivot_longer(cols = starts_with("avg")) %>% 
  ggplot(aes(x = depth, y = value, color = name)) +
  geom_line() +
  facet_grid(rows = vars(class), cols = vars(p), scales = "free") +
  labs(color = "", linetype = "") +
  # scale_color_ob + #scale_linetype_nodesel_varsel +
  xlab("branching tree depth") + ylab("") +
  scale_color_manual(values = setNames(pal[c(1,3,9)], c("avgFacetsLB", "avgFacetsWRays", "avgVertices")), 
                     labels = c("Facets in $\\mathcal{Y}_{N}^{LP}(\\eta)$", "Facets with rays in $\\mathcal{P}_{\\geqq}^{LP}(\\eta)$", "Vertices in $\\mathcal{Y}_{N}^{LP}(\\eta)$")) +
  theme_publish() 
p1
options(tikzMetricPackages = c(
  "\\usepackage[utf8]{inputenc}", 
  "\\usepackage[T1]{fontenc}", 
  "\\usetikzlibrary{calc}",
  "\\usepackage{amssymb}"))
if (copyTabFigs) {
  tikz(file = here(pathOverLeaf, "fig_lb.tex"), standAlone=F, width = 7, height = 5)
  print(p1)
  dev.off()
}

p2 <- dat %>% ungroup() %>% 
  filter(configValSplit == "MOFVREVISITED2", configLB == "WLP") %>% 
  select(class, depth, p, pct) %>% 
  ggplot(aes(x = depth, y = pct)) +
  geom_line() +
  # geom_point() +
  facet_grid(rows = vars(class), cols = vars(p))
p2
if (copyTabFigs) {
  tikz(file = here(pathOverLeaf, "fig_lb_pct.tex"), standAlone=F, width = 7, height = 4)
  print(p2)
  dev.off()
}

dat %>%  
  filter(configValSplit == "MOFVREVISITED2", configLB == "WLP") %>% 
  select(class, configLB, configValSplit, depth, p, avgVerticesFacetsLB) %>% 
  pivot_longer(cols = starts_with("avg")) %>% 
  ggplot(aes(x = depth, y = value, color = name, linetype = configLB)) +
  geom_line() +
  facet_grid(rows = vars(class), cols = vars(p), scales = "free")
```


